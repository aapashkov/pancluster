{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f151cd10790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "from skorch import torch, dataset, NeuralNetClassifier\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genome\n",
       "GCA_000149955.2    [[508, 472, 415, 419, 410, 334, 262, 211, 128,...\n",
       "GCA_000222805.1    [[548, 390, 300, 277, 272, 315, 93, 44, 229, 2...\n",
       "GCA_000259975.2    [[318, 289, 364, 194, 298, 113, 35, 59, 30, 85...\n",
       "GCA_000260175.2    [[377, 372, 379, 381, 376, 361, 2104, 399, 400...\n",
       "GCA_000260215.2    [[89, 91, 18, 21, 2, 337, 366, 13, 8, 66, 54],...\n",
       "                                         ...                        \n",
       "GCA_032878545.1    [[508, 472, 415, 419, 410, 334, 475, 262, 211,...\n",
       "GCA_032991405.1    [[430, 414, 401, 281, 552, 275, 259, 87, 77, 2...\n",
       "GCA_034509825.1    [[430, 414, 551, 401, 281, 275, 259, 87, 77, 2...\n",
       "GCA_036785135.1    [[423, 392, 346, 347, 344, 342, 159, 65, 172, ...\n",
       "GCA_038050555.1    [[430, 414, 401, 281, 552, 275, 259, 87, 77, 2...\n",
       "Name: gene_family, Length: 242, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (\n",
    "    pd.read_csv(\n",
    "        \"../data/pancluster/pancluster.full.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        usecols=[\"genome\", \"gene_family\"],\n",
    "        converters={\"gene_family\": ast.literal_eval}\n",
    "    )\n",
    "    .groupby(\"genome\")\n",
    "    .aggregate(list)[\"gene_family\"]\n",
    "    # Zero will be reserved for padding, so add 1 to every gene family\n",
    "    .apply(lambda x: list(map(lambda y: [i+1 for i in y], x)))\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\n",
    "    \"../accessions.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index_col=\"genome\"\n",
    ")[\"fsp\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanclusterModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_genes: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_size: int,\n",
    "        num_classes: int,\n",
    "        device: torch.device = torch.device(\"cpu\")\n",
    "    ):\n",
    "        super(PanclusterModule, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # The embedding allows us to treat gene families as categories\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=unique_genes,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # The LSTM learns the important features of each BGC\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # Attention weights the importance of each BGC\n",
    "        self.attention = torch.nn.Linear(\n",
    "            in_features=hidden_size,\n",
    "            out_features=1,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # The final layer maps to the classes we wish to predict\n",
    "        self.output = torch.nn.Linear(\n",
    "            in_features=hidden_size,\n",
    "            out_features=num_classes,\n",
    "            device=self.device\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch: torch.Tensor):\n",
    "\n",
    "        # The batch is a zero-padded 3D tensor of shape\n",
    "        # (n_genomes, max_bgc_count, max_bgc_length)\n",
    "\n",
    "        # # In order to train efficiently, we will extract all BGC tensors into\n",
    "        # # a single list\n",
    "        # bgcs = [bgc for genome in batch for bgc in genome]\n",
    "        # bgc_lengths = torch.tensor(list(map(len, bgcs)))\n",
    "\n",
    "        # # LSTMs require that input sequences are sorted by length in descending\n",
    "        # # order. However, we must keep track of the indices used for sorting\n",
    "        # # so that, at a later stage, when splitting tensors to batches, each set\n",
    "        # # of BGCs is restored to its original position\n",
    "        # sorted_indices = torch.argsort(bgc_lengths, descending=True)\n",
    "        # restored_indices = torch.argsort(sorted_indices)\n",
    "        # bgcs = [bgcs[index] for index in sorted_indices]\n",
    "        # bgc_lengths = bgc_lengths[sorted_indices]\n",
    "\n",
    "        # # Because each BGC contains different number of genes, we pad them with\n",
    "        # # zeros, allowing us to create two dimensional tensors\n",
    "        # # Example: [[1 2 3], [1 2]] -> [[1 2 3] [1 2 0]]\n",
    "        # padded_bgcs = torch.nn.utils.rnn.pad_sequence(\n",
    "        #     sequences=bgcs,\n",
    "        #     batch_first=True,\n",
    "        #     padding_value=0\n",
    "        # )\n",
    "\n",
    "        original_shape = batch.shape\n",
    "        padded_bgcs = batch.flatten(0, 1)\n",
    "\n",
    "        # We are now ready to pass the BGCs to the embedding layer\n",
    "        embedded_bgcs: torch.Tensor = self.embedding(padded_bgcs)\n",
    "\n",
    "        # Pack the BGC embeddings\n",
    "        packed_input = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            input=embedded_bgcs,\n",
    "            lengths=bgc_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # Pass packed BGC embeddings through LSTM\n",
    "        packed_output: torch.nn.utils.rnn.PackedSequence = (\n",
    "            self.lstm(packed_input)[0]\n",
    "        )\n",
    "\n",
    "        # Unpack LSTM outputs\n",
    "        padded_output = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=packed_output,\n",
    "            batch_first=True\n",
    "        )[0]\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores: torch.Tensor = self.attention(padded_output)\n",
    "        attention_scores.squeeze_(dim=-1)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(-1)\n",
    "\n",
    "        # Compute the weighted sum of LSTM outputs and restore original order\n",
    "        context_vector = torch.sum(padded_output * attention_weights, dim=1)\n",
    "        context_vector = context_vector[restored_indices]\n",
    "\n",
    "        # Reshape back into batch format\n",
    "        batch_sizes = list(map(len, batch))\n",
    "        instance_vectors = torch.split(context_vector, batch_sizes, dim=0)\n",
    "\n",
    "        # Aggregate BGCs per instance\n",
    "        batch_output = torch.stack([\n",
    "            instance.mean(dim=0) for instance in instance_vectors\n",
    "        ])\n",
    "\n",
    "        # Pass outputs to final fully connected layer\n",
    "        final_output: torch.Tensor = self.output(batch_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "unique_genes = data.explode().explode().max() + 1\n",
    "embedding_dim = 5\n",
    "hidden_size = 10\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 35)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum number of BGCs in a genome\n",
    "max_bgc_count = data.apply(len).max()\n",
    "\n",
    "# Maximum number of genes in a BGC\n",
    "max_bgc_length = data.apply(lambda x: max(map(len, x))).max()\n",
    "max_bgc_count, max_bgc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad genomes with dummy BGCs so all genomes have the same number of BGCs\n",
    "data = data.apply(lambda x: x + [[0]] * (max_bgc_count - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each BGC with dummy genes such that all BGCs have the same length\n",
    "data = data.apply(lambda x: [y + [0] * (max_bgc_length - len(y)) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([242, 57, 35])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(data.to_list(), device=device)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13794, 35, 5])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings=unique_genes,\n",
    "    embedding_dim=embedding_dim,\n",
    "    padding_idx=0,\n",
    "    device=device\n",
    ")\n",
    "embedding(X.flatten(0, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(target.cat.codes.tolist(), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    module=PanclusterModule,\n",
    "    module__unique_genes=unique_genes,\n",
    "    module__embedding_dim=embedding_dim,\n",
    "    module__hidden_size=hidden_size,\n",
    "    module__num_classes=num_classes,\n",
    "    module__device=device,\n",
    "    device=\"cuda\",\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    train_split=dataset.ValidSplit(cv=4, stratified=True),\n",
    "    max_epochs=10,\n",
    "    lr=0.001,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset does not have consistent lengths.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/skorch/net.py:1230\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[0;32m-> 1230\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/skorch/net.py:1189\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_train_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/skorch/net.py:1087\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_training_readiness()\n\u001b[1;32m   1085\u001b[0m epochs \u001b[38;5;241m=\u001b[39m epochs \u001b[38;5;28;01mif\u001b[39;00m epochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs\n\u001b[0;32m-> 1087\u001b[0m dataset_train, dataset_valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_split_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m on_epoch_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_train\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_train,\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_valid\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_valid,\n\u001b[1;32m   1092\u001b[0m }\n\u001b[1;32m   1093\u001b[0m iterator_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(dataset_train, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/skorch/net.py:1672\u001b[0m, in \u001b[0;36mNeuralNet.get_split_datasets\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_split_datasets\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get internal train and validation datasets.\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m \n\u001b[1;32m   1632\u001b[0m \u001b[38;5;124;03m    The validation dataset can be None if ``self.train_split`` is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \n\u001b[1;32m   1671\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1672\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1673\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_split:\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dataset, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/skorch/net.py:1627\u001b[0m, in \u001b[0;36mNeuralNet.get_dataset\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized:\n\u001b[1;32m   1625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m-> 1627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/skorch/dataset.py:160\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, X, y, length)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m len_X \u001b[38;5;241m=\u001b[39m \u001b[43mget_len\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     len_y \u001b[38;5;241m=\u001b[39m get_len(y)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/skorch/dataset.py:82\u001b[0m, in \u001b[0;36mget_len\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     80\u001b[0m len_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(lens)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(len_set) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset does not have consistent lengths.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(len_set)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset does not have consistent lengths."
     ]
    }
   ],
   "source": [
    "net.fit(X=X, y=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
